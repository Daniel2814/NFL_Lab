{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef0039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionDecoder(nn.Module):\n",
    "    def __init__(self, max_targets=9, dropout=0.15):\n",
    "        super(PositionDecoder, self).__init__()\n",
    "        self.max_targets = max_targets\n",
    "        # attention pooling, learn the importances of diffrent frames\n",
    "        self.global_pool = AdditiveAttentionPooling(embed_size=64)\n",
    "        # fcnn in funnel shape, prob need to adjust this when actually training\n",
    "        self.multi_targ_decoders = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(64, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.GELU(), # modern choice of non linearity\n",
    "                nn.Dropout(dropout * 1.5),\n",
    "\n",
    "                nn.Linear(256, 128),\n",
    "                nn.BatchNorm1d(128),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout),\n",
    "\n",
    "                nn.Linear(128, 64),\n",
    "                nn.BatchNorm1d(64),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(dropout * .5),\n",
    "\n",
    "                nn.Linear(64, 2)\n",
    "            ) for _ in range(max_targets)\n",
    "        ])\n",
    "    def forward(self, encoded_sequence, target_mask):\n",
    "        # pooling across all steps in input sequence\n",
    "        pooled, _ = self.global_pool(encoded_sequence) \n",
    "        # predict final position\n",
    "        predictions = []\n",
    "        # predict for every target\n",
    "        for i, decoder in enumerate(self.multi_targ_decoders):\n",
    "            pred = decoder(pooled)  \n",
    "            predictions.append(pred)\n",
    "        \n",
    "       # predicted outputs\n",
    "        predictions = torch.stack(predictions, dim=1)\n",
    "        \n",
    "        # mask null targets\n",
    "        target_mask = target_mask.unsqueeze(-1)  \n",
    "        predictions = predictions * target_mask\n",
    "        \n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd2ae27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DJmoorePOS(nn.Module):\n",
    "    def __init__(self, embed_size, num_layers, dropout, mask, dev='cuda'):\n",
    "        super().__init__()\n",
    "        # downsample cnn\n",
    "        self.cnn = CNN_DownSample()\n",
    "        # cnn output size\n",
    "        cnn_output_size = 64 *14 * 31\n",
    "        # transformer encoder module\n",
    "        self.encoder = TransEncoder(\n",
    "            input_dim=cnn_output_size,\n",
    "            embed_size=embed_size,\n",
    "            num_layers=num_layers,\n",
    "            device=dev,\n",
    "            mask=mask,\n",
    "            dropout=dropout,\n",
    "            max_length=100\n",
    "        )\n",
    "        # decoder, predict output\n",
    "        self.decoder = PositionDecoder(dropout=dropout)\n",
    "        \n",
    "    def forward(self, heatmap_sequence, target_mask):\n",
    "        # batch size, seq\n",
    "        batch_size, seq_len = heatmap_sequence.shape[:2]\n",
    "        # downsample each heatmap through the cnn\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            # select all rows and the t col\n",
    "            frame = heatmap_sequence[:, t]\n",
    "            # extract features from cnn\n",
    "            features = self.cnn(frame)\n",
    "            # keep 0dim, flatten all other features (mult) together\n",
    "            features = features.flatten(1)\n",
    "            # save extracted features\n",
    "            cnn_features.append(features)\n",
    "\n",
    "        # stack extracted features into a sequence\n",
    "        sequence_features = torch.stack(cnn_features, dim=1)\n",
    "        # attention mask, not needed for now because we are \n",
    "        # only prediciting the one frame\n",
    "        mask = torch.ones(batch_size, seq_len, device=heatmap_sequence.device)\n",
    "        # encoding sequence\n",
    "        encoder = self.encoder(sequence_features, mask)\n",
    "        # predict positon\n",
    "        position = self.decoder(encoder, target_mask)\n",
    "        # return position\n",
    "        return position\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de967c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sequences(df_grids, max_seq, max_targets):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    masks = []\n",
    "    # get all offensive players\n",
    "    player_to_predict = df[df['player_to_predict'] == True].groupby('play_id_n')['nfl_id'].unique()\n",
    "    \n",
    "    # get players to predict postions\n",
    "    for play_id in df_grids['play_id_n'].unique():\n",
    "        play_data = df_grids[df_grids['play_id_n'] == play_id].sort_values('frame_id')\n",
    "\n",
    "    # loop through every play\n",
    "    for play_id in df_grids['play_id_n'].unique():\n",
    "\n",
    "        play_data = df_grids[df_grids['play_id_n'] == play_id].sort_values('frame_id')\n",
    "        # padding with sequence length less than max\n",
    "        if len(play_data) < max_seq:\n",
    "            # get players to predict ids\n",
    "            players = player_to_predict[play_id]\n",
    "\n",
    "            # only take max targets\n",
    "            players = players[:max_targets]\n",
    "            num_receivers = len(players)\n",
    "            \n",
    "            # mask of players\n",
    "            mask = torch.zeros(max_targets)\n",
    "            mask[:num_receivers] = 1\n",
    "            \n",
    "            grids = [torch.from_numpy(grid).float() for grid in play_data['grid']]\n",
    "            sequence = torch.stack(grids, dim=0)\n",
    "            \n",
    "            noise = torch.randn_like(sequence) * 0.0001\n",
    "            sequence = sequence + noise\n",
    "            \n",
    "            # out of dist value\n",
    "            padding_needed = max_seq - len(play_data)\n",
    "            if padding_needed > 0:\n",
    "                num_channels = 2 + max_targets + 1\n",
    "                padding = torch.full((padding_needed, num_channels, 55, 121), -1.0)\n",
    "                sequence = torch.cat([sequence, padding], dim=0)\n",
    "        \n",
    "            # get target frame for every player\n",
    "            target_frame = play_data.iloc[-1]['frame_id']\n",
    "            target_positions = torch.zeros(max_targets, 2)\n",
    "            frame_data = df[(df['play_id_n'] == play_id) & (df['frame_id'] == target_frame)]\n",
    "\n",
    "            valid_targets = 0\n",
    "            for i, receiver_id in enumerate(players):\n",
    "                receiver_data = frame_data[frame_data['nfl_id'] == receiver_id]\n",
    "                if not receiver_data.empty:\n",
    "                    x = float(receiver_data['x'].iloc[0]) / 120\n",
    "                    y = float(receiver_data['y'].iloc[0]) / 53.3\n",
    "                    target_positions[i] = torch.tensor([x, y])\n",
    "                    valid_targets += 1\n",
    "            \n",
    "            if valid_targets > 0:\n",
    "                sequences.append(sequence)\n",
    "                targets.append(target_positions)\n",
    "                masks.append(mask)\n",
    "\n",
    "\n",
    "    if len(sequences) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    return (torch.stack(sequences, dim=0), \n",
    "            torch.stack(targets, dim=0), \n",
    "            torch.stack(masks, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdf525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "def train(model, train_loader, val_loader, loss_func, optimizer, scheduler, epochs):\n",
    "    # losses\n",
    "    train_losses = []\n",
    "    # early stopping\n",
    "    es = np.inf\n",
    "    es_count = 0\n",
    "    # training rounds\n",
    "    for epoch in range(epochs):\n",
    "        # set to traing mode\n",
    "        model.train()\n",
    "        # loss and batch count\n",
    "        epoch_loss = 0\n",
    "        batches = 0\n",
    "        # load training sequences and targets\n",
    "        for batch_sequence, batch_targets, batch_masks in train_loader:\n",
    "            batch_sequence = batch_sequence.to('cuda')\n",
    "            batch_targets = batch_targets.to('cuda')\n",
    "            batch_masks = batch_masks.to('cuda')\n",
    "            # zero gradient\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            preds = model(batch_sequence, batch_masks) # predicted value\n",
    "            loss = loss_func(preds, batch_targets, batch_masks) # loss\n",
    "            # backprop time (as an aside its so cool it works so effectively being a realtively simple concept)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1) # prevent exploding gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # batch loss calcs\n",
    "            epoch_loss += loss.item() * batch_sequence.size(0)\n",
    "            batches += batch_sequence.size(0)\n",
    "\n",
    "        # epoch loss on train set\n",
    "        avg_loss = epoch_loss / batches\n",
    "\n",
    "        val_loss = 0\n",
    "        val_batches = 0\n",
    "\n",
    "        # val loss\n",
    "        model.eval() # set to eval mode\n",
    "        with torch.no_grad(): \n",
    "            for val_seq, val_targ, val_mask in val_loader:\n",
    "                # move to cuda\n",
    "                val_seq = val_seq.to('cuda')\n",
    "                val_targ = val_targ.to('cuda')\n",
    "                val_mask = val_mask.to('cuda')\n",
    "                # val_x, val_y\n",
    "                val_preds = model(val_seq, val_mask)\n",
    "                # val loss\n",
    "                val_seq_loss = loss_func(val_preds, val_targ)\n",
    "                val_loss += val_seq_loss.item() * val_seq.size(0) \n",
    "                val_batches += val_seq.size(0)\n",
    "        # loss \n",
    "        val_loss = val_loss / val_batches\n",
    "\n",
    "        # early stopping\n",
    "        if val_loss < es:\n",
    "            es = val_loss\n",
    "            es_count = 0\n",
    "        else:\n",
    "            es_count += 1\n",
    "\n",
    "        # save train loss, update LR \n",
    "        train_losses.append(avg_loss)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'epoch: {epoch} train_loss: {avg_loss} val_loss:{val_loss}')\n",
    "        \n",
    "        if es_count >= 25:\n",
    "            break\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "# predict final positon\n",
    "def predict(model, sequence):\n",
    "    # torch eval mode\n",
    "    model.eval()\n",
    "    # do not compute gradients\n",
    "    with torch.no_grad():\n",
    "        # adds dim if batch dim is not present\n",
    "        if sequence.dim() == 4: \n",
    "            sequence = sequence.unsqueeze(0)\n",
    "        # move to gpu\n",
    "        sequence = sequence.to('cuda')\n",
    "        # prediction\n",
    "        prediction = model(sequence)\n",
    "        x, y = prediction[0].cpu().numpy() # move to cpu, convert from tesnor to numpy\n",
    "        \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03ec831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VarianceRegularizedLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, beta=0.1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  \n",
    "        self.beta = beta    \n",
    "        \n",
    "    def forward(self, predictions, targets):\n",
    "        # mse loss\n",
    "        mse_loss = F.mse_loss(predictions, targets)\n",
    "        \n",
    "        # match the varience between predection and target\n",
    "        pred_var = torch.var(predictions, dim=0)\n",
    "        target_var = torch.var(targets, dim=0)\n",
    "        \n",
    "        # penalize low varience in results \n",
    "        variance_penalty = F.mse_loss(pred_var, target_var)\n",
    "        \n",
    "        return self.alpha * mse_loss + self.beta * variance_penalty\n",
    "\n",
    "class MaskedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, predictions, targets, mask):\n",
    "        # mask\n",
    "        mask = mask.unsqueeze(-1)\n",
    "        # mse\n",
    "        mse = (predictions - targets) **2\n",
    "        masked_mse = mse * mask\n",
    "        # dont calculate \n",
    "        num_valid = mask.sum()\n",
    "        if num_valid > 0:\n",
    "            return masked_mse.sum() / num_valid\n",
    "        else:\n",
    "            return torch.tensor(0.0, device=predictions.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5864d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DJmoorePOS(embed_size=64, num_layers=8, mask=None, dropout=0.15, dev=\"cuda\")\n",
    "# move model to cuda\n",
    "model = model.to(\"cuda\")\n",
    "# standard loss function\n",
    "loss_fun = MaskedMSELoss()\n",
    "# adamW > adam, bc of the proper application of weight decay\n",
    "opti = torch.optim.AdamW(model.parameters(), lr = 0.0001, weight_decay=0.001)\n",
    "# LR scheduler, \n",
    "lr_schedule =  get_cosine_schedule_with_warmup(opti, num_warmup_steps=20, num_training_steps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8417c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq, targ, mask = train_sequences(df_grids, max_seq=81, max_targets=9) \n",
    "\n",
    "X_train, X_test, y_train, y_test, mask_train, mask_test = train_test_split(seq, targ, mask, test_size=0.3, random_state=26, shuffle=True)\n",
    "X_test, X_val, y_test, y_val, mask_test, mask_val = train_test_split(X_test, y_test, mask_test, test_size=0.5, random_state=26, shuffle=True)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train, mask_train)\n",
    "test_dataset = TensorDataset(X_test, y_test, mask_test)\n",
    "val_dataset = TensorDataset(X_val, y_val, mask_val)\n",
    "\n",
    "train_load = DataLoader(train_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "train(model=model, train_loader=train_load, val_loader = val_loader, \n",
    "      loss_func=loss_fun, optimizer=opti,scheduler=lr_schedule, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcf657c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_loop(model, test_loader, loss_fun):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_sample = 0\n",
    "    all_pred = []\n",
    "    all_target = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_seq, batch_targ in test_loader:\n",
    "            batch_seq = batch_seq.to('cuda')\n",
    "            batch_targ = batch_targ.to('cuda')\n",
    "            batch_masks = batch_masks.to('cuda')\n",
    "\n",
    "            preds = model(batch_seq)\n",
    "            loss = loss_fun(preds, batch_targ)\n",
    "\n",
    "            total_loss += loss.item() * batch_seq.size(0) \n",
    "            total_sample += batch_seq.size(0)\n",
    "            \n",
    "            all_pred.append(preds.cpu())\n",
    "            all_target.append(batch_targ.cpu())\n",
    "\n",
    "    avg_test_loss = total_loss / total_sample\n",
    "    all_pred = torch.cat(all_pred, dim = 0)\n",
    "    all_target = torch.cat(all_target, dim=0)\n",
    "\n",
    "    return avg_test_loss, all_pred, all_target\n",
    "\n",
    "loss, pred, target = eval_loop(model=model, test_loader=test_loader, loss_fun=loss_fun)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
