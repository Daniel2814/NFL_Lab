{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "882c941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.chdir('C:/Users/dalto/OneDrive/Pictures/Documents/Emory/NFL Lab/data')\n",
    "df = pd.read_csv('2023_tracking.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5960ee90",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_ids = df['play_id_n'].unique()[:1500]\n",
    "df = df[df['play_id_n'].isin(play_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40823073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "max_targets = df.groupby(['play_id_n', 'frame_id'])['player_to_predict'].sum().max()\n",
    "print(max_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3b0c43",
   "metadata": {},
   "source": [
    "#### 2d Grid of Cords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c7bdcf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pixel_map(player_data, target_player_ids, max_targets, grid_width=121, grid_height=55, sigma=.8):\n",
    "    \n",
    "    num_channels = 2 + max_targets + 1\n",
    "    # three channels, one for offense, one for defense, one for ball location, one for player to predict\n",
    "    pixel_map = np.zeros((num_channels, grid_height, grid_width), dtype=np.float32)\n",
    "    \n",
    "    x_vals = player_data['x'].values\n",
    "    y_vals = player_data['y'].values\n",
    "    \n",
    "    for i, (x, y) in enumerate(zip(x_vals, y_vals)):\n",
    "        x_min = max(0, int(np.floor(x - 2*sigma)))\n",
    "        x_max = min(grid_width, int(np.ceil(x + 2*sigma)) + 1)\n",
    "        y_min = max(0, int(np.floor(y - 2*sigma)))\n",
    "        y_max = min(grid_height, int(np.ceil(y + 2*sigma)) + 1)\n",
    "        \n",
    "        for xi in range(x_min, x_max):\n",
    "            for yi in range(y_min, y_max):\n",
    "                dist_sq = (xi - x)**2 + (yi - y)**2\n",
    "                weight = np.exp(-dist_sq / (2 * sigma**2))\n",
    "\n",
    "                player_id = player_data.iloc[i].get('nfl_id', None)\n",
    "                \n",
    "                if player_id in target_player_ids:\n",
    "                    target_idx = target_player_ids.index(player_id)\n",
    "                    if target_idx < max_targets: # to prevent error\n",
    "                        pixel_map[2 + target_idx, yi, xi] += weight\n",
    "                elif player_data.iloc[i]['player_side'] == 'Offense':\n",
    "                    pixel_map[0, yi, xi] += weight\n",
    "                elif player_data.iloc[i]['player_side'] == 'Defense':\n",
    "                    pixel_map[1, yi, xi] += weight\n",
    "                \n",
    "    \n",
    "    ball_x = player_data['ball_land_x'].iloc[0]\n",
    "    ball_y = player_data['ball_land_y'].iloc[0]\n",
    "    \n",
    "    ball_x_min = max(0, int(np.floor(ball_x - 2*sigma)))\n",
    "    ball_x_max = min(grid_width, int(np.ceil(ball_x + 2*sigma)) + 1)\n",
    "    ball_y_min = max(0, int(np.floor(ball_y - 2*sigma)))\n",
    "    ball_y_max = min(grid_height, int(np.ceil(ball_y + 2*sigma)) + 1)\n",
    "    \n",
    "    for xi in range(ball_x_min, ball_x_max):\n",
    "        for yi in range(ball_y_min, ball_y_max):\n",
    "            dist_sq = (xi - ball_x)**2 + (yi - ball_y)**2\n",
    "            weight = np.exp(-dist_sq / (2 * sigma**2))\n",
    "            pixel_map[3, yi, xi] += weight\n",
    "    \n",
    "    return pixel_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb8c7f",
   "metadata": {},
   "source": [
    "testing one play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a45368c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalto\\AppData\\Local\\Temp\\ipykernel_25552\\2474093878.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_grids_t = df_play_id.groupby(['play_id_n','frame_id']).apply(\n"
     ]
    }
   ],
   "source": [
    "df_play_id = df[df['play_id_n'] == 456]\n",
    "target_player_ids = df_play_id[df_play_id['player_to_predict'] == True]['nfl_id'].unique().tolist()\n",
    "df_grids_t = df_play_id.groupby(['play_id_n','frame_id']).apply(\n",
    "    lambda x: pd.Series({'grid': pixel_map(x, target_player_ids, max_targets)})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84373868",
   "metadata": {},
   "source": [
    "grid for all plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbb8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grids = df.groupby(['play_id_n','frame_id']).apply(\n",
    "    lambda x: pd.Series({'grid': pixel_map(x,  x[x['player_to_predict'] == True]['nfl_id'].unique().tolist(), max_targets)})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9189e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grids = df_grids.reset_index()\n",
    "df_grids = df_grids.sort_values(['play_id_n', 'frame_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe42dca",
   "metadata": {},
   "source": [
    "visual test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e0b800",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_grid = df_grids_t['grid'].iloc[30]\n",
    "\n",
    "# Count how many target player channels have data\n",
    "num_targets = 0\n",
    "for i in range(max_targets):\n",
    "    if sample_grid[2 + i].sum() > 0:\n",
    "        num_targets += 1\n",
    "\n",
    "# Create subplots: 2 base channels + ball + target players\n",
    "total_plots = 3 + num_targets\n",
    "cols = 4\n",
    "rows = (total_plots + cols - 1) // cols\n",
    "\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(14, 5*rows))\n",
    "axes = axes.flatten() if total_plots > 1 else [axes]\n",
    "\n",
    "# Plot offense\n",
    "axes[0].imshow(sample_grid[0], origin='lower', cmap='Reds')\n",
    "axes[0].set_title('Offense Players')\n",
    "axes[0].set_xlabel('X (yards)')\n",
    "axes[0].set_ylabel('Y (yards)')\n",
    "\n",
    "# Plot defense\n",
    "axes[1].imshow(sample_grid[1], origin='lower', cmap='Blues')\n",
    "axes[1].set_title('Defense Players')\n",
    "axes[1].set_xlabel('X (yards)')\n",
    "axes[1].set_ylabel('Y (yards)')\n",
    "\n",
    "# Plot ball location (at index 2 + max_targets)\n",
    "axes[2].imshow(sample_grid[2 + max_targets], origin='lower', cmap='Purples')\n",
    "axes[2].set_title('Ball Landing Location')\n",
    "axes[2].set_xlabel('X (yards)')\n",
    "axes[2].set_ylabel('Y (yards)')\n",
    "\n",
    "# Plot each target player (channels 2 through 2+max_targets-1)\n",
    "plot_idx = 3\n",
    "for i in range(max_targets):\n",
    "    if sample_grid[2 + i].sum() > 0:\n",
    "        axes[plot_idx].imshow(sample_grid[2 + i], origin='lower', cmap='Greens')\n",
    "        axes[plot_idx].set_title(f'Target Player {i+1}')\n",
    "        axes[plot_idx].set_xlabel('X (yards)')\n",
    "        axes[plot_idx].set_ylabel('Y (yards)')\n",
    "        plot_idx += 1\n",
    "\n",
    "for idx in range(total_plots, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e99f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_grids.to_pickle(\"full_grids_1500.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adae6a33",
   "metadata": {},
   "source": [
    "### Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cc55cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalto\\anaconda3\\envs\\nflLab\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2172013ac90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "df_grids = pd.read_pickle(\"full_grids_1500.pkl\")\n",
    "torch.manual_seed(26)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6635c9",
   "metadata": {},
   "source": [
    "#### Global Input Seq Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6455d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_DownSample(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        input_chan = 2 + max_targets + 1 \n",
    "        \n",
    "        # using stride rather than max pooling preforms better as max pooling tends to compress feat. too much.\n",
    "        self.heatmap_encoder = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_chan, out_channels=16, kernel_size=3, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(16), # normalize var and mean\n",
    "            nn.GELU(), # preforms better on average idk if itll make a difference in this application\n",
    "\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.GELU(),\n",
    "\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride = 2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.heatmap_encoder(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f884d25",
   "metadata": {},
   "source": [
    "attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4da6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, mask, dropout=0.15):\n",
    "        super().__init__() # inhert from parent class\n",
    "        \n",
    "        if d_model % nhead != 0:\n",
    "            raise ValueError(f\"d_model ({d_model}) must be divisible by nhead ({nhead})\")\n",
    "            \n",
    "        self.d_model = d_model # dimension of model\n",
    "        self.nhead = nhead # number of attention heads, multi headed \n",
    "        self.head_dim = d_model // nhead \n",
    "\n",
    "        # create key query and values\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        # learn context as a product of the attention heads\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # dropout as a form of regularzation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # scaling function\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, _ = x.shape # batch and length\n",
    "        \n",
    "        # create q, k, v values | init just random matrix mults, learned parameter\n",
    "        qkv = self.qkv_proj(x)\n",
    "        \n",
    "        # split key, query, and value vectors into diff pares\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        \n",
    "        # transpose the matrix so that batch and nhead are treated as batches and self attention is calculated from there\n",
    "        q = q.view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "    \n",
    "        # scaled dot product, scale so values arent 0 or 1 \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale # matrix mult\n",
    "        \n",
    "        # set masked values to -inf so softmax does not \"give\" attention to them\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(1) \n",
    "            mask = mask.expand(B, self.nhead, L, L) \n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "            \n",
    "            \n",
    "        # softmax to give attention weights to each token\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # drop some weights \n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # context vector for a given input sequence\n",
    "        context = torch.matmul(attn_weights, v) \n",
    "        \n",
    "        # transpose so the matrix is in the correct size to be concatinated\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
    "        \n",
    "        # \"combine\" the outputs from the head to one general vector\n",
    "        output = self.out_proj(context)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee94df07",
   "metadata": {},
   "source": [
    "transformer block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445638fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead=4, mask=None, dropout=0.15):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # self attention class definied above\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, nhead=nhead, dropout=dropout, mask=mask)\n",
    "        \n",
    "        # feed forward network for each token\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(64, 128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout), # to combat overfitting\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        # normilzations so values are between 0-1, learned gamma and beta parameters\n",
    "        # to shift center and var for values.\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # standard dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        residual = x\n",
    "        # normalized pre attention layer, gradients flow black directly without the normalizing effecting x values\n",
    "        norm_x = self.norm1(x)\n",
    "        # self attention\n",
    "        attn_output = self.self_attn(norm_x, mask)\n",
    "        # adding residual back to self attention\n",
    "        x = residual + self.dropout(attn_output)\n",
    "        \n",
    "        residual = x\n",
    "        # normalize values\n",
    "        # we do so because over the amount of layers scale can get distorted, lead to super big or small values\n",
    "        norm_x = self.norm2(x) \n",
    "        # basic fcn\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        # adding residual back so that the gradient can flow directly back.\n",
    "        # adds a 1 + terms to gradients, helps solve the vanishing gradients problem\n",
    "        x = residual + self.dropout(ff_output)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e6e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sinusoidal functions, simpler than learned values and generalizes better to unseen parameters\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, max_length=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # droput\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # create matrix\n",
    "        pe = torch.zeros(max_length, embed_size)\n",
    "        \n",
    "        # position tensor shape\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # div_term tensor shape\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "        \n",
    "        # apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # apply cos to odd indices\n",
    "        if embed_size % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # register as buffer so it moves with model to device\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe_slice = self.pe[:x.size(1), :].to(x.device)\n",
    "        x = x + pe_slice\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_size, num_layers, device, dropout, mask, max_length):\n",
    "        super(TransEncoder, self).__init__()\n",
    "        self.embed_size = embed_size \n",
    "        self.device = device\n",
    "        # learned matrix projection\n",
    "        self.input_projection = nn.Linear(input_dim, embed_size)\n",
    "        # postional encoding \n",
    "        self.position_encoding = PositionalEncoding(embed_size, dropout, max_length)\n",
    "        # layers of model, just stacked encoding layer\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    mask=mask,\n",
    "                    nhead=4, # number of attention heads\n",
    "                    dropout=dropout\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # normalize after attention\n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # input layer matrix mult\n",
    "        projected_input = self.input_projection(x)\n",
    "        # position encodings\n",
    "        out = self.position_encoding(projected_input)\n",
    "        # pass through transformer block\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, mask)\n",
    "        # normalize gradients\n",
    "        out = self.norm(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d1449e",
   "metadata": {},
   "source": [
    "##### Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fada4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dropout, embedding, nhead):\n",
    "        super().__init__()\n",
    "        # attention layers\n",
    "        self.attention_self = MultiHeadAttention(d_model=embedding, nhead=4, mask=None, dropout=dropout)\n",
    "        # cross atten\n",
    "        self.cross_attention = MultiHeadAttention(d_model=embedding, nhead=4, mask=None, dropout=dropout)\n",
    "        # layer normal\n",
    "        self.norm1 = nn.LayerNorm(embedding)\n",
    "        self.norm2 = nn.LayerNorm(embedding)\n",
    "        self.norm3 = nn.LayerNorm(embedding)\n",
    "        # droput\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # fcnn\n",
    "        self.fcnn = nn.Sequential(\n",
    "            nn.Linear(embedding, embedding*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(embedding*2, embedding)\n",
    "        )\n",
    "    # decoder forward pass\n",
    "    def forward(self, decoder_input, encoded_context, target_mask):\n",
    "        # self attention amoung decoder \n",
    "        residual = decoder_input\n",
    "        norm_x = self.norm1(decoder_input)\n",
    "        self_attn = self.attention_self(norm_x, mask = target_mask)\n",
    "        decoder_input = residual + self.dropout(self_attn)\n",
    "\n",
    "        # cross attention to encoder\n",
    "        norm_x = self.norm2(decoder_input)\n",
    "        cross_atn = self.encoder_cross_attention(norm_x, encoded_context)\n",
    "        decoder_input += self.dropout(cross_atn)\n",
    "\n",
    "        # fcnn predictions\n",
    "        norm_x = self.norm3(decoder_input)\n",
    "        ffcn = self.fcnn(norm_x)\n",
    "        out = decoder_input + self.dropout(ffcn)\n",
    "\n",
    "        return out\n",
    "    \n",
    "    def encoder_cross_attention(self, query, key_value):\n",
    "        B, L_q, _ = query.shape\n",
    "        B, L_kv, _ = key_value.shape\n",
    "        \n",
    "        # Q from decoder, kv form encoder, using key value from encoder\n",
    "        q = self.cross_attention.qkv_proj(query)[:, :, :self.cross_attention.d_model]\n",
    "        kv = self.cross_attention.qkv_proj(key_value)[:, :, self.cross_attention.d_model:]\n",
    "        k, v = kv.chunk(2, dim=-1)\n",
    "        \n",
    "        # attention calc\n",
    "        q = q.view(B, L_q, self.cross_attention.nhead, self.cross_attention.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, L_kv, self.cross_attention.nhead, self.cross_attention.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, L_kv, self.cross_attention.nhead, self.cross_attention.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.cross_attention.scale\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.cross_attention.dropout(attn_weights)\n",
    "        \n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L_q, self.cross_attention.d_model)\n",
    "        \n",
    "        return self.cross_attention.out_proj(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc6e89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_size, dropout, max_length=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # droput\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # create matrix\n",
    "        pe = torch.zeros(max_length, embed_size)\n",
    "        \n",
    "        # position tensor shape\n",
    "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # div_term tensor shape\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-torch.log(torch.tensor(10000.0)) / embed_size))\n",
    "        \n",
    "        # apply sin to even indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # apply cos to odd indices\n",
    "        if embed_size % 2 == 1:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term[:-1])\n",
    "        else:\n",
    "            pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # register as buffer so it moves with model to device\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe_slice = self.pe[:x.size(1), :].to(x.device)\n",
    "        x = x + pe_slice\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca8aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransDecoder(nn.Module):\n",
    "    def __init__(self, target_mask, embedding, dropout, nhead, layers, max_targets, max_seq_len):\n",
    "        super(TransDecoder, self).__init__()\n",
    "        self.max_targets = max_targets\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # postional embeddings\n",
    "        self.pos_embed = PositionalEncoding(embed_size=embedding, dropout=0.15, max_length=150)\n",
    "\n",
    "        # decode \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(embedding=embedding, dropout=dropout, nhead=nhead)\n",
    "            for _ in range(layers)\n",
    "        ])\n",
    "        # predictions\n",
    "        self.multi_decoder = nn.ModuleList([\n",
    "            nn.Linear(embedding, 2)\n",
    "            for _ in range(max_targets)\n",
    "        ])\n",
    "        # target queries\n",
    "        self.target_queries = nn.Parameter(torch.randn(max_targets, embedding))\n",
    "\n",
    "        # normalization\n",
    "        self.norm = nn.LayerNorm(embedding)\n",
    "        \n",
    "    def forward(self, encoded_context, target_mask, future_steps):\n",
    "        # batch size and device\n",
    "        batch_size = encoded_context.shape[0]\n",
    "        device = encoded_context.device\n",
    "\n",
    "        # learnable targets \n",
    "        decoder_input = self.target_queries.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # outputs\n",
    "        outputs = []\n",
    "        current_input = decoder_input\n",
    "\n",
    "        # output sequence\n",
    "        for step in range(future_steps):\n",
    "            # add positional context\n",
    "            pos_input = current_input + self.pos_embed.pe[step, :self.embedding].unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "            # init \n",
    "            decoded = pos_input\n",
    "\n",
    "            # through transformer layers\n",
    "            for layer in self.layers:\n",
    "                decoded = layer(decoded, encoded_context, target_mask)\n",
    "            \n",
    "            # norm to prevent exploding gradients\n",
    "            decoded = self.norm(decoded)\n",
    "            \n",
    "            # positons for timestep\n",
    "            step_predictions = []\n",
    "            for i, decoder in enumerate(self.multi_decoder):\n",
    "                pred = decoder(decoded[:, i, :])\n",
    "                step_predictions.append(pred)\n",
    "            # stack all predictions\n",
    "            step_output = torch.stack(step_predictions, dim=1)\n",
    "            outputs.append(step_output)\n",
    "\n",
    "            current_input = decoded\n",
    "        \n",
    "        predictions = torch.stack(outputs, dim=2)\n",
    "\n",
    "        # mask\n",
    "        target_mask_expanded = target_mask.unsqueeze(-1).unsqueeze(-1)\n",
    "        predictions = predictions * target_mask_expanded\n",
    "\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1c1648",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq_2_seq(df_grids, max_targets, input_seq_len, output_seq_len):\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    masks = []\n",
    "    # get all offensive players\n",
    "    player_to_predict = df[df['player_to_predict'] == True].groupby('play_id_n')['nfl_id'].unique()\n",
    "    \n",
    "    # get players to predict postions\n",
    "    for play_id in df_grids['play_id_n'].unique():\n",
    "        play_data = df_grids[df_grids['play_id_n'] == play_id].sort_values('frame_id')\n",
    "\n",
    "    # loop through every play\n",
    "    for play_id in df_grids['play_id_n'].unique():\n",
    "\n",
    "        play_data = df_grids[df_grids['play_id_n'] == play_id].sort_values('frame_id')\n",
    "        # padding with sequence length less than max\n",
    "        if len(play_data) >= input_seq_len + output_seq_len:\n",
    "            # get players to predict ids\n",
    "            players = player_to_predict[play_id]\n",
    "\n",
    "            # only take max targets\n",
    "            players = players[:max_targets]\n",
    "            num_receivers = len(players)\n",
    "            \n",
    "            # mask of players\n",
    "            mask = torch.zeros(max_targets)\n",
    "            mask[:num_receivers] = 1\n",
    "            \n",
    "            # input sequence\n",
    "            input_grids = [torch.from_numpy(grid).float() for grid in play_data['grid'].iloc[:input_seq_len]]\n",
    "            sequence = torch.stack(input_grids, dim=0)\n",
    "            \n",
    "            # add noise for regularzatiom\n",
    "            noise = torch.randn_like(sequence) * 0.0001\n",
    "            input_sequence = sequence + noise\n",
    "            \n",
    "            # target sequence\n",
    "            target_positions = torch.zeros(max_targets, output_seq_len, 2)\n",
    "\n",
    "            # append postions for every targeted reciver\n",
    "            for step in range(output_seq_len):\n",
    "                frame_idx = input_seq_len + step\n",
    "                if frame_idx < len(play_data):\n",
    "                    target_frame = play_data.iloc[frame_idx]['frame_id']\n",
    "                    frame_data = df[(df['play_id_n'] == play_id) & (df['frame_id'] == target_frame)]\n",
    "                    \n",
    "                    for i, receiver_id in enumerate(players):\n",
    "                        receiver_data = frame_data[frame_data['nfl_id'] == receiver_id]\n",
    "                        if not receiver_data.empty:\n",
    "                            x = float(receiver_data['x'].iloc[0]) / 120\n",
    "                            y = float(receiver_data['y'].iloc[0]) / 53.3\n",
    "                            target_positions[i, step] = torch.tensor([x, y])\n",
    "\n",
    "            # if there is targets\n",
    "            if num_receivers > 0:\n",
    "                sequences.append(input_sequence)\n",
    "                targets.append(target_positions)\n",
    "                masks.append(mask)\n",
    "\n",
    "\n",
    "    if len(sequences) == 0:\n",
    "        return None, None, None\n",
    "    \n",
    "    return (torch.stack(sequences, dim=0), \n",
    "            torch.stack(targets, dim=0), \n",
    "            torch.stack(masks, dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_mask(seq_len, device):\n",
    "    # create a mask where feature postions cannot be seen during training\n",
    "    mask = torch.triu(torch.ones(seq_len, seq_len, device=device), diagonal=1)\n",
    "\n",
    "def padding_mask(lengths, max_len, device):\n",
    "    batch_size = len(lengths)\n",
    "    mask = torch.zeros(batch_size, max_len, device=device, dtype=torch.bool)\n",
    "    for i, length in enumerate(lengths):\n",
    "        mask[i, :length] = True\n",
    "    return mask\n",
    "\n",
    "class MaskedSequenceLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, predictions, targets, target_mask):\n",
    "        mask = target_mask.unsqueeze(-1).unsqueeze(-1)\n",
    "        mask = mask.expand_as(predictions)\n",
    "\n",
    "        mse = (predictions - targets) ** 2\n",
    "        # dont consider error on padding, masked tokens etc\n",
    "        masked_mse = mse * mask\n",
    "\n",
    "        # normalize for masked elements \n",
    "        valid = mask.sum()\n",
    "        if valid > 0:\n",
    "            return masked_mse.sum() / valid\n",
    "        else:\n",
    "            return torch.tensor(0.0, device=predictions.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1faa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DJMooreSeq(nn.Module):\n",
    "    def __init__(self, embed_size, encoder_layers, decoder_layers, \n",
    "                 max_targets, dropout, nheads, dev='cuda') -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        # general vars\n",
    "        self.embedding_size = embed_size\n",
    "        self.max_targets = max_targets\n",
    "        self.device = dev\n",
    "\n",
    "        # context cnn\n",
    "        self.context_cnn = CNN_DownSample()\n",
    "        context_cnn_output = 64* 14 * 31\n",
    "\n",
    "        # transformer encoder\n",
    "        self.encoder = TransEncoder(input_dim=context_cnn_output,\n",
    "                                    embed_size=embed_size,\n",
    "                                    num_layers=encoder_layers,\n",
    "                                    device=dev,\n",
    "                                    mask=None,\n",
    "                                    dropout=dropout,\n",
    "                                    max_length=100)\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = TransDecoder(target_mask=None,\n",
    "                                    embedding=embed_size,\n",
    "                                    dropout=dropout,\n",
    "                                    nhead=nheads,\n",
    "                                    layers=decoder_layers,\n",
    "                                    max_targets=max_targets,\n",
    "                                    max_seq_len=100)\n",
    "    def forward():\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nflLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
