{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0779c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, dropout, embedding, nhead):\n",
    "        super().__init__()\n",
    "        # attention layers\n",
    "        self.attention_self = MultiHeadAttention(d_model=embedding, nhead=nhead, mask=None, dropout=dropout)\n",
    "\n",
    "        # cross atten, query values, info\n",
    "        self.cross_q_proj = nn.Linear(embedding, embedding)\n",
    "        self.cross_k_proj = nn.Linear(embedding, embedding)\n",
    "        self.cross_v_proj =  nn.Linear(embedding, embedding)\n",
    "        self.cross_out_proj = nn.Linear(embedding, embedding)\n",
    "\n",
    "        self.nhead = nhead\n",
    "        self.head_dim = embedding // nhead\n",
    "        self.scale = self.head_dim ** -0.5 # 1/sqrt(dk)\n",
    "\n",
    "        # layer normal\n",
    "        self.norm1 = nn.LayerNorm(embedding)\n",
    "        self.norm2 = nn.LayerNorm(embedding)\n",
    "        self.norm3 = nn.LayerNorm(embedding)\n",
    "\n",
    "        # droput\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # fcnn\n",
    "        self.fcnn = nn.Sequential(\n",
    "            nn.Linear(embedding, embedding*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(embedding*2, embedding)\n",
    "        )\n",
    "    # decoder forward pass\n",
    "    def forward(self, decoder_input, encoded_context, casual_mask):\n",
    "\n",
    "        # self attention amoung decoder\n",
    "        residual = decoder_input\n",
    "        norm_x = self.norm1(decoder_input)\n",
    "        self_attn = self.attention_self(norm_x, casual_mask)\n",
    "        decoder_input = residual + self.dropout(self_attn)\n",
    "\n",
    "        # cross attention to encoder\n",
    "        norm_x = self.norm2(decoder_input)\n",
    "        cross_atn = self.encoder_cross_attention(norm_x, encoded_context)\n",
    "\n",
    "        # dropout, also cant do inplace ops bc of backprop\n",
    "        decoder_input = decoder_input + self.dropout(cross_atn)\n",
    "\n",
    "        # fcnn predictions\n",
    "        norm_x = self.norm3(decoder_input)\n",
    "        ffcn = self.fcnn(norm_x)\n",
    "        out = decoder_input + self.dropout(ffcn)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def encoder_cross_attention(self, query, key_value):\n",
    "        B, L_q, _ = query.shape # decoder input\n",
    "        B, L_kv, _ = key_value.shape # encoder output\n",
    "\n",
    "        # both values full percision\n",
    "        query = query.float()\n",
    "        key_value = key_value.float()\n",
    "\n",
    "        q = self.cross_q_proj(query)\n",
    "        k = self.cross_k_proj(key_value)\n",
    "        v = self.cross_v_proj(key_value)\n",
    "\n",
    "        q = q.view(B, L_q, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, L_kv, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, L_kv, self.nhead, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L_q, -1)\n",
    "\n",
    "        output = self.cross_out_proj(context)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd7b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, mask, dropout=0.15):\n",
    "        super().__init__() # inhert from parent class\n",
    "\n",
    "        if d_model % nhead != 0:\n",
    "            raise ValueError(f\"d_model ({d_model}) must be divisible by nhead ({nhead})\")\n",
    "\n",
    "        self.d_model = d_model # dimension of model\n",
    "        self.nhead = nhead # number of attention heads, multi headed\n",
    "        self.head_dim = d_model // nhead\n",
    "\n",
    "        # create key query and values\n",
    "        self.qkv_proj = nn.Linear(d_model, 3 * d_model)\n",
    "        # learn context as a product of the attention heads\n",
    "        self.out_proj = nn.Linear(d_model, d_model)\n",
    "        # dropout as a form of regularzation\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        # scaling function\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        B, L, _ = x.shape # batch and length\n",
    "\n",
    "        # create q, k, v values | init just random matrix mults, learned parameter\n",
    "        x_input = x.float() if x.dtype != torch.float32 else x\n",
    "        qkv = self.qkv_proj(x_input) # use full percison for attention calculations\n",
    "\n",
    "        # split key, query, and value vectors into diff pares\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # transpose the matrix so that batch and nhead are treated as batches and self attention is calculated from there\n",
    "        q = q.view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        k = k.view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "        v = v.view(B, L, self.nhead, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        # scaled dot product, scale so values arent 0 or 1\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale # matrix mult\n",
    "\n",
    "        # set masked values to -inf so softmax does not \"give\" attention to them\n",
    "        if mask is not None:\n",
    "          if mask.dim() == 2:\n",
    "            mask = mask.expand(B, self.nhead, L, L)\n",
    "          elif mask.dim() == 3:\n",
    "            mask = mask.unsqueeze(1)\n",
    "            mask = mask.expand(B, self.nhead, L, L)\n",
    "          elif mask.dim() == 4:\n",
    "            mask = mask.expand(B, self.nhead, L, L)\n",
    "\n",
    "          scores = scores.masked_fill(mask == 0, torch.finfo(scores.dtype).min) # ignore masked values\n",
    "\n",
    "        # softmax to give attention weights to each token\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # drop some weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # context vector for a given input sequence\n",
    "        context = torch.matmul(attn_weights, v)\n",
    "\n",
    "        # transpose so the matrix is in the correct size to be concatinated\n",
    "        context = context.transpose(1, 2).contiguous().view(B, L, self.d_model)\n",
    "\n",
    "        # \"combine\" the outputs from the head to one general vector\n",
    "        output = self.out_proj(context)\n",
    "\n",
    "        return output.to(x.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead=None, mask=None, dropout=0.15):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        # self attention class definied above\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, nhead=nhead, dropout=dropout, mask=mask)\n",
    "\n",
    "        # feed forward network for each token\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model*2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout), # to combat overfitting\n",
    "            nn.Linear(d_model*2, d_model)\n",
    "        )\n",
    "\n",
    "        # normilzations so values are between 0-1, learned gamma and beta parameters\n",
    "        # to shift center and var for values.\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # standard dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        residual = x\n",
    "        # normalized pre attention layer, gradients flow black directly without the normalizing effecting x values\n",
    "        norm_x = self.norm1(x)\n",
    "        # self attention\n",
    "        attn_output = self.self_attn(norm_x, mask)\n",
    "        # adding residual back to self attention\n",
    "        x = residual + self.dropout(attn_output)\n",
    "\n",
    "        residual = x\n",
    "        # normalize values\n",
    "        # we do so because over the amount of layers scale can get distorted, lead to super big or small values\n",
    "        norm_x = self.norm2(x)\n",
    "        # basic fcn\n",
    "        ff_output = self.feed_forward(norm_x)\n",
    "        # adding residual back so that the gradient can flow directly back.\n",
    "        # adds a 1 + terms to gradients, helps solve the vanishing gradients problem\n",
    "        x = residual + self.dropout(ff_output)\n",
    "\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
